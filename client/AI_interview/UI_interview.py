import streamlit as st
import os
import numpy as np
import librosa
import scipy.ndimage
import tempfile

# ============================
# 0ï¸âƒ£ API í‚¤ ì„¤ì •
# ============================
# Google GenAI
GOOGLE_API_KEY = os.environ.get("GOOGLE_API_KEY", "AIzaSyDuI6DK3v17kGqqSyM4uHRWoC2qRC-Kzpg")
# ============================
# 1ï¸âƒ£ Whisper STT
# ============================

import whisper
import torch
device = "cuda" if torch.cuda.is_available() else "cpu"
model = whisper.load_model("small")  # small/medium/large ì„ íƒ

def run_whisper(audio_file):
    result = model.transcribe(audio_file, language="ko")
    return result["text"]
print("í˜„ì¬ ì¥ì¹˜:", device)
# ============================
# 2ï¸âƒ£ ëª©ì†Œë¦¬ ì•ˆì •ì„± ë¶„ì„
# ============================

import numpy as np
import librosa
import scipy.ndimage

def analyze_stability(audio_file):
    y, sr = librosa.load(audio_file, sr=16000)

    # --- Pitch ë¶„ì„ ---
    pitches, magnitudes = librosa.piptrack(y=y, sr=sr)
    pitch_values = pitches[pitches > 0]
    if len(pitch_values) == 0: 
        pitch_values = np.array([1.0])
    pitch_values = scipy.ndimage.median_filter(pitch_values, size=3)

    jitter_std = np.std(pitch_values) / np.mean(pitch_values)
    jitter_range = (np.percentile(pitch_values, 95) - np.percentile(pitch_values, 5)) / np.mean(pitch_values)
    jitter = 0.7 * jitter_std + 0.3 * jitter_range

    # --- ë³¼ë¥¨ ë¶„ì„ ---
    rms = librosa.feature.rms(y=y, frame_length=2048, hop_length=512)[0]
    volume_values = rms[rms > 0]
    if len(volume_values) == 0: 
        volume_values = np.array([1.0])
    volume_values = scipy.ndimage.median_filter(volume_values, size=3)

    shimmer_std = np.std(volume_values) / np.mean(volume_values)
    shimmer_range = (np.percentile(volume_values, 95) - np.percentile(volume_values, 5)) / np.mean(volume_values)
    shimmer = 0.7 * shimmer_std + 0.3 * shimmer_range

    # --- ì•ˆì •ì„± ì§€í‘œ ---
    instability = 0.5*jitter + 0.5*shimmer

    # ê³ ì • ë²”ìœ„ (ì‹¤í—˜ì ìœ¼ë¡œ 0~5.0)
    inst_min, inst_max = 0.0, 5.0

    score = 10 * (1 - (instability - inst_min) / (inst_max - inst_min))
    score = round(max(0, min(10, score)), 1)

    print(f"instability={instability:.4f}, score={score}")

    return score


def label_from_stability(score):
    if score >= 6: return "ì•ˆì •ì  âœ…","success"
    elif score >= 3: return "ë³´í†µ âš ï¸","warning"
    else: return "ë¶ˆì•ˆì • âŒ","error"

# ============================
# 3ï¸âƒ£ ìì„¸ ë¶„ì„
# ============================
import xml.etree.ElementTree as ET

import xml.etree.ElementTree as ET

def parse_posture_summary(xml_path):
    tree = ET.parse(xml_path)
    root = tree.getroot()

    head_tilt_count = 0
    body_tilt_count = 0
    gesture_count = 0
    frame_total = 0

    for frame in root.findall("frame"):
        frame_total += 1
        analysis = frame.find("analysis")
        if analysis is None:
            continue

        for result in analysis.findall("result"):
            rtype = result.get("type", "")
            if rtype == "head_tilt":
                head_tilt_count += 1
            elif rtype == "body_tilt":
                body_tilt_count += 1
            elif rtype == "gesture":
                gesture_count += 1

    # ê¸°ë³¸ ë¼ë²¨ë§
    labels = []
    if head_tilt_count > frame_total * 0.3:
        labels.append("ë¨¸ë¦¬ ê¸°ìš¸ì„ ì¦ìŒ")
    if body_tilt_count > frame_total * 0.3:
        labels.append("ëª¸ ê¸°ìš¸ì–´ì§ ì¦ìŒ")

    # ì œìŠ¤ì²˜ ë¼ë²¨ë§
    if gesture_count == 0:
        gesture_label = "ì œìŠ¤ì²˜ ì—†ìŒ (ë‹µë³€ì´ ë”±ë”±í•  ìˆ˜ ìˆìŒ)"
    elif 1 <= gesture_count <= 3:
        gesture_label = "ìì—°ìŠ¤ëŸ¬ìš´ ì œìŠ¤ì²˜ ì‚¬ìš©"
    else:
        gesture_label = "ì œìŠ¤ì²˜ ê³¼ë‹¤ (ì‚°ë§Œí•  ìˆ˜ ìˆìŒ)"

    labels.append(gesture_label)

    return {
        "frames": frame_total,
        "head_tilt_count": head_tilt_count,
        "body_tilt_count": body_tilt_count,
        "gesture_count": gesture_count,
        "label": ", ".join(labels)
    }


# ============================
# 4ï¸âƒ£ Google GenAI + LangChain
# ============================
from langchain.prompts import ChatPromptTemplate
from langchain.chains import LLMChain
from langchain_google_genai import ChatGoogleGenerativeAI

llm = ChatGoogleGenerativeAI(
    model="gemini-2.5-flash",
    temperature=0.6,
    api_key=GOOGLE_API_KEY
)

prompt = ChatPromptTemplate.from_messages([
    ("system", "ë„ˆëŠ” ì „ë¬¸ ë©´ì ‘ê´€ ì—­í• ì„ ë§¡ëŠ”ë‹¤.  ì•„ë˜ëŠ” ë©´ì ‘ìì˜ í–‰ë™ ë¶„ì„ ë°ì´í„°ì´ë‹¤. ì´ ë°ì´í„°ì—ëŠ” ë©´ì ‘ ì¤‘ ê°ì§€ëœ ìì„¸, í‘œì •, ìŒì„±ì˜ íŠ¹ì§•ì´ í¬í•¨ëœë‹¤"),
    ("human", """
ë©´ì ‘ ë‹µë³€ ë¶„ì„:
- Transcript: {transcript}
- ëª©ì†Œë¦¬ ì•ˆì •ì„± ì§€ìˆ˜: {voice_stability}
- ìì„¸ ë° ì œìŠ¤ì²˜ í™œìš©: {posture}

ìš”êµ¬ì‚¬í•­:
1. ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë©´ì ‘ìì˜ í–‰ë™ì„ **ê°ê´€ì ìœ¼ë¡œ ìš”ì•½**í•œë‹¤. (ìˆ«ì, íšŸìˆ˜ í¬í•¨)  
2. ë©´ì ‘ê´€ì˜ ì…ì¥ì—ì„œ ë©´ì ‘ìì—ê²Œ ë„ì›€ì´ ë  **í”¼ë“œë°±ì„ 2~3ë¬¸ì¥**ìœ¼ë¡œ ì œì‹œí•œë‹¤.  
   - í”¼ë“œë°±ì€ ê¸ì •ì ì¸ ë¶€ë¶„ì„ ë¨¼ì € ì§šê³ , ê°œì„ í•  ì ì„ ì œì•ˆí•œë‹¤.  
   - ë„ˆë¬´ ê³µê²©ì ì´ì§€ ë§ê³  **ì¹œì ˆí•˜ê³  ì „ë¬¸ì ì¸ í†¤**ì„ ìœ ì§€í•œë‹¤.  
3. í”¼ë“œë°±ì€ **ê°„ê²°í•˜ê³  ì‹¤ìš©ì ìœ¼ë¡œ** ì‘ì„±í•œë‹¤.  
     
ì¶œë ¥ í˜•ì‹:
- ë¶„ì„ ìš”ì•½: ...
- ê°•ì : ...
- í”¼ë“œë°±: ...
""")
])

chain = LLMChain(llm=llm, prompt=prompt)

# ============================
# streamlit ì‹¤í–‰
# ============================
st.set_page_config(page_title="AI ë©´ì ‘ê´€", page_icon="ğŸ¤", layout="wide")

# íƒ€ì´í‹€ + ì„œë¸Œíƒ€ì´í‹€
st.markdown(
    """
    <h1 style='text-align: center; color: #1E90FF; font-size: 40px;'>
        ğŸ¤ AI ë©´ì ‘ê´€
    </h1>
    <h3 style='text-align: center; color: #555;'>
        ë‹¹ì‹ ì˜ <b>ëª©ì†Œë¦¬ Â· ìì„¸ Â· í‘œì •</b>ì„ ì¢…í•© ë¶„ì„í•©ë‹ˆë‹¤
    </h3>
    """,
    unsafe_allow_html=True
)


st.divider()

# ----------------------
# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
# ----------------------
if "chapters" not in st.session_state:
    st.session_state.chapters = []  # ì „ì²´ ë©´ì ‘ ê¸°ë¡ (ì„¸ì…˜ë³„)
if "history" not in st.session_state:
    st.session_state.history = []   # í˜„ì¬ ì„¸ì…˜ ê¸°ë¡

# ----------------------
# ìƒˆ ë©´ì ‘ ì„¸ì…˜ ì‹œì‘ ë²„íŠ¼
# ----------------------
if "uploader_key" not in st.session_state:
    st.session_state.uploader_key = 0
if "posture_key" not in st.session_state:
    st.session_state.posture_key = 0

if st.button("ğŸ†• ìƒˆë¡œìš´ ë©´ì ‘ ì‹œì‘"):
    if st.session_state.history:
        st.session_state.chapters.append(st.session_state.history)
    st.session_state.history = []
    st.session_state.uploader_key += 1  # key ë°”ê¿”ì¤Œ
    st.session_state.posture_key += 1
    st.rerun()


# ì„œë¹„ìŠ¤ ì†Œê°œ ì¹´ë“œ
st.markdown(
    """
    <div style="
        background-color:#f0f8ff;
        padding:20px;
        border-radius:12px;
        text-align:center;
        border: 1px solid #1E90FF;
        margin-bottom:15px;
    ">
        <h3 style="color:#1E90FF;">ğŸ™ï¸ ë©´ì ‘ ë‹µë³€ ì—…ë¡œë“œ</h3>
        <p style="color:#333;">ì§€ì›ìì˜ ìŒì„±ê³¼ ìì„¸ë¥¼ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.<br>
        ì§€ì› í˜•ì‹: <b>WAV, M4A, MP3, FLAC, XML</b></p>
    </div>
    """,
    unsafe_allow_html=True
)
uploaded_file = st.file_uploader(
    "ğŸ™ï¸ ìŒì„± ë°ì´í„°",
    type=["wav","m4a","mp3","flac"],
    key=f"file_uploader_{st.session_state.uploader_key}"  # key ë§¤ë²ˆ ë°”ë€œ
)

posture_file = st.file_uploader(
    "ğŸ§ ìì„¸ ë°ì´í„°",
    type=["xml"],
    key=f"posture_uploader_{st.session_state.posture_key}" # key ë§¤ë²ˆ ë°”ë€œ
)
summary = None
if posture_file:
    with tempfile.NamedTemporaryFile(delete=False, suffix=".xml") as tmp_xml:
        tmp_xml.write(posture_file.getbuffer())
        summary = parse_posture_summary(tmp_xml.name)  # ìì„¸ ë¶„ì„ í•¨ìˆ˜ ì‚¬ìš©

if uploaded_file and posture_file:
    st.success(f"âœ… íŒŒì¼ ì—…ë¡œë“œ ì™„ë£Œ: {uploaded_file.name, posture_file.name}")
    st.audio(uploaded_file)
    # ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥ 
    with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as tmp_file:
        tmp_file.write(uploaded_file.getbuffer())
        tmp_path = tmp_file.name

    with st.spinner("ë¶„ì„ ì¤‘..."):
        transcript = run_whisper(tmp_path)
        score = analyze_stability(tmp_path)
        voice_label, color = label_from_stability(score)
        feedback = chain.run({
            "transcript": transcript,
            "voice_stability": voice_label,
            "posture" : summary["label"]
        })
        # ê²°ê³¼ ì €ì¥
        result = {
            "transcript": transcript,
            "score": score,
            "voice_label": voice_label,
            "color": color,
            "posture": summary["label"] if summary else "ë°ì´í„° ì—†ìŒ",
            #"expression": expression_label,
            "feedback": feedback
        }
        st.session_state.history.append(result)


def render_question_result(i, res):
    st.markdown(f"### â“ ë‹µë³€ {i}")
    st.info(res["transcript"])

    col1, col2, = st.columns([1, 2])
    with col1:
        st.metric("ëª©ì†Œë¦¬ ì•ˆì •ì„± ì ìˆ˜", f"{res['score']:.2f}/10")
    with col2:
        st.progress(float(res["score"])/10)

    if res["color"] == "success":
        st.success(f"ëª©ì†Œë¦¬ ì•ˆì •ì„±: {res['voice_label']}")
    elif res["color"] == "warning":
        st.warning(f"ëª©ì†Œë¦¬ ì•ˆì •ì„±: {res['voice_label']}")
    else:
        st.error(f"ëª©ì†Œë¦¬ ì•ˆì •ì„±: {res['voice_label']}")

    if "posture" in res:
        if "ì•ˆì •ì " in res["posture"]:
            st.success(f" ìì„¸: {res['posture']}")
        elif "ë¶ˆì•ˆì •" in res["posture"] or "ê¸°ìš¸ì–´ì§" in res["posture"]:
            st.error(f" ìì„¸: {res['posture']}")
        else:
            st.warning(f" ìì„¸: {res['posture']}")

    st.success(f"{res['feedback']}")
    st.divider()


# ---------------- ê²°ê³¼ ì¶œë ¥ ----------------
if st.session_state.history or st.session_state.chapters:
    st.subheader("ğŸ“‚ ë©´ì ‘ ê¸°ë¡")

    # âœ… í˜„ì¬ ì„¸ì…˜
    if st.session_state.history:
        st.markdown("## ğŸš€ í˜„ì¬ ì§„í–‰ì¤‘ì¸ ë©´ì ‘")
        for i, res in enumerate(st.session_state.history, 1):
            render_question_result(i, res)

    # âœ… ê³¼ê±° ì„¸ì…˜ (expanderë¡œ ì ‘ê¸°)
    for c_idx, chapter in enumerate(st.session_state.chapters, 1):
        with st.expander(f"ğŸ“Œ ê³¼ê±° ë©´ì ‘ ì„¸ì…˜ {c_idx}", expanded=False):
            for i, res in enumerate(chapter, 1):
                render_question_result(i, res)


# ----------------------
# í‘¸í„°
# ----------------------

st.divider()
st.markdown(
    """
    <div style='text-align: center; color: gray; font-size: 14px;'>
        Â© 2025 AI Interview Project | Powered by Whisper Â· LangChain Â· Hailo
    </div>
    """,
    unsafe_allow_html=True
)