import streamlit as st
import os
import numpy as np
import librosa
import scipy.ndimage

# ============================
# 0ï¸âƒ£ API í‚¤ ì„¤ì •
# ============================
# Google GenAI
GOOGLE_API_KEY = os.environ.get("GOOGLE_API_KEY", "AIzaSyDuI6DK3v17kGqqSyM4uHRWoC2qRC-Kzpg")
# ============================
# 1ï¸âƒ£ Whisper STT
# ============================

import whisper
import torch
device = "cuda" if torch.cuda.is_available() else "cpu"
model = whisper.load_model("small")  # small/medium/large ì„ íƒ

def run_whisper(audio_file):
    result = model.transcribe(audio_file, language="ko")
    return result["text"]
print("í˜„ì¬ ì¥ì¹˜:", device)
# ============================
# 2ï¸âƒ£ ëª©ì†Œë¦¬ ì•ˆì •ì„± ë¶„ì„
# ============================
def analyze_voice(audio_file):
    """
    ìŒì„± íŒŒì¼ì„ ë¶„ì„í•˜ì—¬ í”¼ì¹˜(Jitter), ë³¼ë¥¨(Shimmer), ì•ˆì •ì„±(Stability) ì ìˆ˜ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
    ì ìˆ˜ëŠ” ìì—°ìŠ¤ëŸ¬ìš´ ë©´ì ‘ í”¼ë“œë°± ê¸°ì¤€ìœ¼ë¡œ 0~10ì  ì²™ë„ë¡œ í™˜ì‚°ë©ë‹ˆë‹¤.
    """
    # ì˜¤ë””ì˜¤ ë¡œë“œ
    y, sr = librosa.load(audio_file, sr=16000)

    # ------------------ í”¼ì¹˜ ------------------
    pitches, magnitudes = librosa.piptrack(y=y, sr=sr)
    pitch_values = pitches[pitches > 0]
    if len(pitch_values) == 0:
        pitch_values = np.array([1.0])
    pitch_values = scipy.ndimage.median_filter(pitch_values, size=5)

    jitter = np.std(pitch_values) / np.mean(pitch_values)

    # ------------------ ë³¼ë¥¨ ------------------
    rms = librosa.feature.rms(y=y, frame_length=2048, hop_length=512)[0]
    volume_values = rms[rms > 0]
    if len(volume_values) == 0:
        volume_values = np.array([1.0])
    volume_values = scipy.ndimage.median_filter(volume_values, size=5)

    shimmer = np.std(volume_values) / np.mean(volume_values)

    # ------------------ ì•ˆì •ì„± ì ìˆ˜ ------------------
    stability_raw = 1 / (1 + jitter + shimmer)
    stability_score = round(max(0, min(10, stability_raw * 10)), 1)
    return stability_score

# ============================
# 3ï¸âƒ£ Google GenAI + LangChain
# ============================
from langchain.prompts import ChatPromptTemplate
from langchain.chains import LLMChain
from langchain_google_genai import ChatGoogleGenerativeAI

llm = ChatGoogleGenerativeAI(
    model="gemini-2.5-flash",
    temperature=0.7,
    api_key=GOOGLE_API_KEY
)

prompt = ChatPromptTemplate.from_messages([
    ("system", "ë„ˆëŠ” ë©´ì ‘ê´€ì´ì•¼. ëª©ì†Œë¦¬ ë–¨ë¦¼, ì•ˆì •ì„±ì„ ê¸°ë°˜ìœ¼ë¡œ ì˜ ëŒ€ë‹µí•˜ê³  ìˆëŠ”ì§€ í‰ê°€í•´."),
    ("human", """
ë©´ì ‘ ë‹µë³€ ë¶„ì„:
- Transcript: {transcript}
- ëª©ì†Œë¦¬ ì•ˆì •ì„± ì§€ìˆ˜: {stability_score:.2f}

ìœ„ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ëª©ì†Œë¦¬ ì•ˆì •ì„±, ë–¨ë¦¼ ì •ë„, ìì‹ ê° ì—¬ë¶€ë¥¼ í‰ê°€í•˜ê³ ,
í”¼ë“œë°± ì½”ë©˜íŠ¸ë¥¼ ê°„ë‹¨í•˜ê²Œ ì‘ì„±í•´ì¤˜.
     
ì¶œë ¥í˜•ì‹ì€ ë‘ ì¤„ì •ë„ë¡œ í•´ì¤˜ 
""")
])

chain = LLMChain(llm=llm, prompt=prompt)

# ============================
# 4ï¸âƒ£ ì‹¤í–‰
# ============================
if __name__ == "__main__":
    audio_path = "test1.m4a"  # ë¶„ì„í•  ë©´ì ‘ ìŒì„± íŒŒì¼

    print("ğŸ™ï¸ Whisperë¡œ ìŒì„± â†’ í…ìŠ¤íŠ¸ ë³€í™˜ ì¤‘...")
    transcript = run_whisper(audio_path)

    print("ë³€í™˜ëœ í…ìŠ¤íŠ¸:", transcript)

    print("ğŸ“Š ìŒì„± ì•ˆì •ì„± ë¶„ì„ ì¤‘...")
    stability_score = analyze_voice(audio_path)

    print("ğŸ’¡ Google LLM í”¼ë“œë°± ìƒì„± ì¤‘...")
    feedback = chain.run({
    "transcript": transcript,
    "stability_score": stability_score  # ê·¸ëƒ¥ ìˆ«ì
})

    print("\n=== ìµœì¢… ë©´ì ‘ í”¼ë“œë°± ===\n")
    print(feedback)
