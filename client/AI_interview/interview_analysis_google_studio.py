# íŒŒì¼: interview_analysis_google_studio.py
import torch
import torchaudio
import numpy as np
from transformers import WhisperProcessor, WhisperForConditionalGeneration
from google import genai


# ----------------- 1. Google AI Studio API í‚¤ ì„¤ì • -----------------
client = genai.Client(api_key="AIzaSyDuI6DK3v17kGqqSyM4uHRWoC2qRC-Kzpg")

# ----------------- 2. ì˜¤ë””ì˜¤ ë¶ˆëŸ¬ì˜¤ê¸° -----------------
def load_audio(file_path, target_sr=16000):
    waveform, sr = torchaudio.load(file_path)
    if sr != target_sr:
        waveform = torchaudio.functional.resample(waveform, orig_freq=sr, new_freq=target_sr)
        sr = target_sr
    return waveform.squeeze(0).numpy(), sr

# ----------------- 3. ìŒì„± ë¶„ì„ (Pitch, Volume) -----------------
def analyze_audio(waveform, sr=16000, frame_size=4096, hop_size=2048):
    # RMS ë³¼ë¥¨ ê³„ì‚°
    volume = waveform ** 2
    avg_vol = np.mean(volume)
    vol_var = np.std(volume)
    
    # í”„ë ˆì„ ë‹¨ìœ„ í”¼ì¹˜ ê³„ì‚°
    def autocorr_pitch_frame(sig):
        pitches = []
        for start in range(0, len(sig)-frame_size, hop_size):
            frame = sig[start:start+frame_size]
            frame = frame - np.mean(frame)
            corr = np.correlate(frame, frame, mode='full')
            corr = corr[len(corr)//2:]
            d = np.diff(corr)
            start_idx = np.where(d > 0)[0]
            if len(start_idx) == 0:
                pitches.append(0)
                continue
            peak = np.argmax(corr[start_idx[0]:]) + start_idx[0]
            pitch = sr / peak if peak != 0 else 0
            pitches.append(pitch)
        return np.array(pitches)
    
    pitch_values = autocorr_pitch_frame(waveform)
    avg_pitch = np.mean(pitch_values[pitch_values>0])  # 0 ì œì™¸
    pitch_var = np.std(pitch_values[pitch_values>0])
    
    return {
        "avg_pitch": avg_pitch,
        "pitch_var": pitch_var,
        "avg_vol": avg_vol,
        "vol_var": vol_var
    }

# ----------------- 4. Whisperë¡œ ìŒì„± â†’ í…ìŠ¤íŠ¸ -----------------
def transcribe_audio(file_path):
    processor = WhisperProcessor.from_pretrained("openai/whisper-small")
    model = WhisperForConditionalGeneration.from_pretrained("openai/whisper-small")
    
    waveform, sr = load_audio(file_path)
    input_features = processor(waveform, sampling_rate=sr, return_tensors="pt").input_features
    predicted_ids = model.generate(input_features)
    transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]
    return transcription

# ----------------- 5. Google LLM í”¼ë“œë°± -----------------
def generate_feedback(transcript, analysis):
    # 1ï¸âƒ£ LLMì—ê²Œ ì „ë‹¬í•  prompt ì •ì˜
    prompt = f"""
ë©´ì ‘ ë‹µë³€ ë¶„ì„:
- Transcript: {transcript}
- Pitch í‰ê· : {analysis['avg_pitch']:.2f} Hz, ë³€ë™: {analysis['pitch_var']:.2f}
- ë³¼ë¥¨ í‰ê· : {analysis['avg_vol']:.3f}, ë³€ë™: {analysis['vol_var']:.3f}

ìœ„ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë©´ì ‘ ë‹µë³€ì˜ ëª©ì†Œë¦¬ í†¤, ì•ˆì •ì„±, ìì‹ ê° ë“±ì„ í‰ê°€í•˜ê³ ,
1~10ì  ì²™ë„ë¡œ ì ìˆ˜ì™€ ì½”ë©˜íŠ¸ë¥¼ ì‘ì„±í•´ì¤˜.
"""

    # 2ï¸âƒ£ ìµœì‹  1.39.1 SDK ê¸°ì¤€ generate_content ì‚¬ìš©
    response = client.models.generate_content(
        model="gemini-2.5-flash",
        contents=prompt
    )

    # 3ï¸âƒ£ ê²°ê³¼ í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
    feedback = response.text
    print("=== LLM Feedback ===\n", feedback)
    return feedback

# ----------------- 6. ì‹¤í–‰ -----------------
if __name__ == "__main__":
    audio_file = "sample.flac"  # í…ŒìŠ¤íŠ¸ìš© ìŒì„± íŒŒì¼ ê²½ë¡œ
    
    print("ğŸ¤ ì˜¤ë””ì˜¤ ë¶„ì„ ì¤‘...")
    waveform, sr = load_audio(audio_file)
    analysis = analyze_audio(waveform)
    
    print("ğŸ“ Whisperë¡œ í…ìŠ¤íŠ¸ ë³€í™˜ ì¤‘...")
    transcript = transcribe_audio(audio_file)
    
    print("ğŸ’¡ Google LLM í”¼ë“œë°± ìƒì„± ì¤‘...")
    feedback = generate_feedback(transcript, analysis)
    
    print("\n=== Transcript ===")
    print(transcript)
    print("\n=== Voice Analysis ===")
    for k, v in analysis.items():
        print(f"{k}: {v:.2f}")
    print("\n=== LLM Feedback ===")
    print(feedback)
